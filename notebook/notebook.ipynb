{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = 'C:/Program Files/Java/jdk-21'\n",
    "os.environ['SPARK_HOME'] = 'C:/Program Files/spark'\n",
    "os.environ['HADOOP_HOME'] = 'C:/Program Files/winutils'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrameExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los datos (una lista de tuplas)\n",
    "data = [\n",
    "    (\"Ana\", 25, \"Madrid\"),\n",
    "    (\"Luis\", 30, \"Barcelona\")\n",
    "]\n",
    "\n",
    "# Crear el DataFrame sin especificar el esquema (lo inferirá automáticamente)\n",
    "df = spark.createDataFrame(data, [\"nombre\", \"edad\", \"ciudad\"])\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df.select(\n",
    "    F.translate(F.regexp_replace(F.upper(F.col('alcaldia')), \"[.,©@#³$%&/()=!¡\\?¿]\", \"\"), 'ÁÉÍÓÚÃ', 'AEIOUA').alias('aux')).dropDuplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.select('*',\n",
    "           F.when(F.col('aux').like(\"%XO%\"), \"XOCHIMILCO\")\n",
    "            .otherwise(F.when(F.col('aux').like(\"%MAGDA%\"), \"LA MAGDALENA CONTRERAS\")\n",
    "                        .otherwise(F.when(F.col('aux').like(\"%COYO%\"), \"COYOACAN\")\n",
    "                                    .otherwise(F.when(F.col('aux').like(\"%POTZA%\"), \"AZCAPOTZALCO\")\n",
    "                                                .otherwise(F.when(F.col('aux').like(\"%TLAL%\"), \"TLALPAN\")\n",
    "                                                            .otherwise(F.when(F.col('aux').like(\"%TLAH%\"), \"TLAHUAC\")\n",
    "                                                                        .otherwise(F.when(F.col('aux').like(\"%CUAJ%\"), \"CUAJIMALPA DE MORELOS\")\n",
    "                                                                                    .otherwise(F.when((F.col('aux').like(\"%BENITO%\")) | (F.col('aux').like(\"%JUAREZ%\")), \"BENITO JUAREZ\")\n",
    "                                                                                                .otherwise(F.when(F.col('aux').like(\"%MILPA%\"), \"MILPA ALTA\")\n",
    "                                                                                                            .otherwise(F.when(F.col('aux').like(\"%TAPALA%\"), \"IZTAPALAPA\")\n",
    "                                                                                                                        .otherwise(F.when(F.col('aux').like(\"%CALCO%\"), \"IZTACALCO\")\n",
    "                                                                                                                                    .otherwise(F.when( (F.col('aux').like(\"%MOC%\")) | (F.col('aux').like(\"%CONDESA%\")) | (F.col('aux').like(\"%ROMA%\")), \"CUAUHTEMOC\")\n",
    "                                                                                                                                                .otherwise(F.when(F.col('aux').like(\"%MADERO%\"), \"GUSTAVO A MADERO\")\n",
    "                                                                                                                                                            .otherwise(F.when(F.col('aux').like(\"%VENUSTIANO%\"), \"VENUSTIANO CARRANZA\")\n",
    "                                                                                                                                                                        .otherwise(F.when( (F.col('aux').like(\"%MIGUEL%\")) | (F.col('aux').like(\"%POLANCO%\")) , \"MIGUEL HIDALGO\")\n",
    "                                                                                                                                                                                    .otherwise(F.when(F.col('aux').like(\"%ALVARO%\"), \"ALVARO OBREGON\")\n",
    "                                                                                                                                                                                                .otherwise(None)))))))))))))))).alias('prueba')).na.drop(how=\"any\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.when((F.col('aux').like(\"%BENITO%\")) | (F.col('aux').like(\"%JUAREZ%\")), \"BENITO JUAREZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux2 = df.select(\n",
    "    F.translate(F.regexp_replace(F.upper(F.col('categoria')), \"[.,©@#³$%&/()=!¡\\?¿]\", \"\"), 'ÁÉÍÓÚÃ', 'AEIOUA').alias('aux')).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aux2.select('*',\n",
    "           F.when(F.col('aux').like(\"%INCOMPLETA%\"), \"DOCUMENTACION INCOMPLETA\")\n",
    "            .otherwise(F.when(F.col('aux').like(\"%NO TENER LICENCIA%\"), \"DOCUMENTACION INCOMPLETA\")\n",
    "                        .otherwise(F.when(F.col('aux').like(\"%MAL USO%\"), \"USO INCORRECTO DE LA VIA PUBLICA\")\n",
    "                                    .otherwise(F.when(F.col('aux').like(\"%ABANDONO%\"), \"USO INCORRECTO DE LA VIA PUBLICA\")\n",
    "                                                .otherwise(F.when(F.col('aux').like(\"%ESTACIONAR%\"), \"USO INCORRECTO DE LA VIA PUBLICA\")\n",
    "                                                            .otherwise(F.when(F.col('aux').like(\"%EXCEDER%\"), \"EXCESO DE VELOCIDAD\")\n",
    "                                                                        .otherwise(F.when(F.col('aux').like(\"%EFECTOS%\"), \"CONDUCIR BAJO INFLUENCIA\")\n",
    "                                                                                    .otherwise(F.when(F.col('aux').like(\"%NO RESPETAR%\"), \"INFRACCIONES DE TRANSITO\")\n",
    "                                                                                                .otherwise(F.when(F.col('aux').like(\"%VIOLACIONES DE VEH%\"), \"INFRACCIONES DE TRANSITO\")\n",
    "                                                                                                            .otherwise(F.when(F.col('aux').like(\"%INMOVILIZACIÓN%\"), \"INFRACCIONES DE TRANSITO\")\n",
    "                                                                                                                        .otherwise(F.when(F.col('aux').like(\"%INCUMPLIR%\"), \"SEGURIDAD VIAL\")\n",
    "                                                                                                                                    .otherwise(F.when(F.col('aux').like(\"%VIOLAR NORMAS%\"), \"SEGURIDAD VIAL\")\n",
    "                                                                                                                                                .otherwise(F.when(F.col('aux').like(\"%PONER EN RIESGO%\"), \"SEGURIDAD VIAL\")\n",
    "                                                                                                                                                            .otherwise('OTROS'))))))))))))).alias('prueba'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.select('prueba').dropDuplicates().show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('marca_general').dropDuplicates().show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('submarca').dropDuplicates().show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('color').dropDuplicates().show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALVARO OBREGON\n",
    "AZCAPOTZALCO\n",
    "BENITO JUAREZ\n",
    "COYOACAN\n",
    "CUAJIMALPA DE MORELOS\n",
    "CUAUHTEMOC\n",
    "GUSTAVO A. MADERO\n",
    "IZTACALCO\n",
    "IZTAPALAPA\n",
    "LA MAGDALENA CONTRERAS\n",
    "MIGUEL HIDALGO\n",
    "MILPA ALTA\n",
    "TLAHUAC\n",
    "TLALPAN\n",
    "VENUSTIANO CARRANZA\n",
    "XOCHIMILCO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip setuptools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'notebook/data/data.csv'\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = \"notebook/data/infracciones_infracciones_transito_2023_b6.csv\"\n",
    "file_path2 = \"notebook/data/infracciones_infracciones_transito_2024_b1.csv\"\n",
    "\n",
    "\n",
    "df1 = spark.read.csv(file_path1, header=True, inferSchema=True, encoding=\"ISO-8859-1\")\n",
    "df2 = spark.read.csv(file_path2, header=True, inferSchema=True, encoding=\"ISO-8859-1\")\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(F.col(\"id_infraccion\").cast(T.StringType()).alias(\"id_infraccion\"),\n",
    "               \"fecha_infraccion\",\n",
    "               F.col(\"ao_infraccion\").cast(T.IntegerType()).alias(\"anio_infraccion\"),\n",
    "               F.col(\"mes\").cast(T.StringType()).alias(\"mes\"),\n",
    "               F.col(\"categoria\").cast(T.StringType()).alias(\"categoria\"),\n",
    "               F.col(\"articulo\").cast(T.StringType()).alias(\"articulo\"),\n",
    "               F.col(\"fraccion\").cast(T.StringType()).alias(\"fraccion\"),\n",
    "               F.col(\"inciso\").cast(T.StringType()).alias(\"inciso\"),\n",
    "               F.col(\"parrafo\").cast(T.StringType()).alias(\"parrafo\"),\n",
    "               F.col(\"placa\").cast(T.StringType()).alias(\"placa\"),\n",
    "               F.col(\"Color\").cast(T.StringType()).alias(\"color\"),\n",
    "               F.col(\"marca_general\").cast(T.StringType()).alias(\"marca_general\"),\n",
    "               F.col(\"submarca\").cast(T.StringType()).alias(\"submarca\"),\n",
    "               F.col(\"en_la_calle\").cast(T.StringType()).alias(\"calle1\"),\n",
    "               F.col(\"entre_calle\").cast(T.StringType()).alias(\"calle2\"),\n",
    "               F.col(\"y_calle\").cast(T.StringType()).alias(\"calle3\"),\n",
    "               F.col(\"colonia\").cast(T.StringType()).alias(\"colonia\"),\n",
    "               F.col(\"alcaldia\").cast(T.StringType()).alias(\"alcaldia\"),\n",
    "               F.col(\"longitud\").cast(T.IntegerType()).alias(\"longitud\"),\n",
    "               F.col(\"latitud\").cast(T.IntegerType()).alias(\"latitud\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_categoricas = [\"mes\", \"categoria\", \"id_infraccion\", \"fraccion\", \"inciso\", \"parrafo\",\n",
    "               \"placa\", \"color\", \"marca_general\", \"submarca\", \"calle1\",\n",
    "               \"calle2\", \"calle3\", \"colonia\", \"alcaldia\", \"articulo\"]\n",
    "\n",
    "var_numericas = [\"anio_infraccion\",\"longitud\", \"latitud\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in var_categoricas:\n",
    "    df = df.withColumn(col, F.translate(F.regexp_replace(F.upper(F.col(col)), \"[.,!¡\\?¿]\", \"\"), 'ÁÉÍÓÚ', 'AEIOU'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop latiud y longitud\n",
    "df = df.drop(*['latitud', 'longitud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"notebook/data/poligonos.csv\"\n",
    "\n",
    "poligonos =  spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poligonos.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(poligonos, how=\"inner\", on=[\"colonia\", \"alcaldia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"fecha_infraccion\", F.to_date(F.col(\"fecha_infraccion\"), \"dd/MM/yyyy\"))\\\n",
    "       .select('*',\n",
    "                F.dayofmonth(F.col(\"fecha_infraccion\")).alias(\"dia\"),\n",
    "                F.month(F.col(\"fecha_infraccion\")).alias(\"mes_numero\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cantidad de nulos\n",
    "def cantidad_nulos(df, threshold, squema):\n",
    "    df = df.select(squema)\n",
    "    primer_fila = df.select([((F.sum(F.col(c).isNull().cast(T.IntegerType())) / df.count())).alias(c) for c in df.columns]).first()\n",
    "    primer_fila_dict = {col: primer_fila[col] for col in df.columns if ((primer_fila[col]<=threshold) & (primer_fila[col]>0.0))}\n",
    "    return list(primer_fila_dict.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad_nulos(train, 0.6, var_numericas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas_imputation = cantidad_nulos(train, 0.6, var_categoricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Frequent Category Imputation over training set\n",
    "mode_values = {}\n",
    "for col in categoricas_imputation:\n",
    "    mode_value = train.filter(F.col(col).isNotNull()).groupBy(col).count().orderBy(F.desc(\"count\")).first()[0]\n",
    "    mode_values[col] = mode_value\n",
    "\n",
    "for col in categoricas_imputation:\n",
    "    train = train.withColumn(col, F.when(F.col(col).isNull(), mode_values[col]).otherwise(F.col(col)))\n",
    "    test = test.withColumn(col, F.when(F.col(col).isNull(), mode_values[col]).otherwise(F.col(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Sample Imputation over training set\n",
    "import random\n",
    "sample_dict=[]\n",
    "for column in categoricas_imputation:\n",
    "    distinct_values = train.select(column).dropna().distinct().toPandas()[\"color\"].tolist()\n",
    "    sample_value = random.choice(distinct_values)\n",
    "    \n",
    "    #sample_dict[column] = distinct_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRECUENCY ENCODING\n",
    "\n",
    "def frecuency_encoding(df, vars):\n",
    "\n",
    "    total_count = df.count()\n",
    "\n",
    "    for col in vars:\n",
    "        freq_df = df.groupBy(col).agg((F.count(\"*\") / total_count).alias(f\"{col}_frequency_encoded\"))\n",
    "\n",
    "        name = col + '_frequency_encoded'\n",
    "        \n",
    "        df = df.join(freq_df, on=col, how=\"left\").drop(col).withColumnRenamed(name, col)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "def frequency_encoding(df, vars):\n",
    "\n",
    "    for col in vars:\n",
    "        # Definir la ventana para particionar por la columna\n",
    "        window = Window.partitionBy(col)\n",
    "        \n",
    "        # Calcular la frecuencia de cada valor en la columna\n",
    "        freq_col = F.count(\"*\").over(window) / F.count(\"*\").over(Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))        \n",
    "        # Añadir la columna codificada por frecuencia\n",
    "        df = df.withColumn(f\"{col}_frequency_encoded\", freq_col)\n",
    "\n",
    "        return df\n",
    "        \n",
    "        # Eliminar la columna original\n",
    "        df = df.drop(col).withColumnRenamed(f\"{col}_frequency_encoded\", col)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = frequency_encoding(train, var_categoricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyEncoder:\n",
    "    def __init__(self):\n",
    "        self.freq_cols = {}\n",
    "    \n",
    "    def fit(self, df, vars):\n",
    "        \"\"\"\n",
    "        Ajusta el encoder calculando las frecuencias para las columnas categóricas.\n",
    "        \"\"\"\n",
    "        for col in vars:\n",
    "            window = Window.partitionBy(col)\n",
    "            \n",
    "            freq_col = F.count(\"*\").over(window) / F.count(\"*\").over(Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))\n",
    "            \n",
    "            self.freq_cols[col] = freq_col\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Aplica la codificación de frecuencia al conjunto de datos.\n",
    "        \"\"\"\n",
    "        for col, freq_col in self.freq_cols.items():\n",
    "            df = df.withColumn(f\"{col}_frequency_encoded\", freq_col)\n",
    "        \n",
    "            df = df.drop(col).withColumnRenamed(f\"{col}_frequency_encoded\", col)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, df, vars):\n",
    "        \"\"\"\n",
    "        Ajusta y transforma el conjunto de datos de entrenamiento.\n",
    "        \"\"\"\n",
    "        self.fit(df, vars)\n",
    "        return self.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_categoricas = [\"mes\", \"placa\", \"color\", \"marca_general\", \"submarca\", \"calle1\",\n",
    "               \"calle2\", \"calle3\", \"colonia\", \"alcaldia\", \"articulo\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = FrequencyEncoder()\n",
    "train_encoded = encoder.fit_transform(train, var_categoricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = encoder.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"color\", \"marca_general\", \"calle1\",\n",
    "               \"calle2\", \"calle3\", \"colonia\", \"alcaldia\",\"mes\", \"dia\", \"anio_infraccion\"]\n",
    "\n",
    "y = \"categoria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Crear el StringIndexer para convertir la columna 'categoria' en la variable 'label'\n",
    "indexer = StringIndexer(inputCol=y, outputCol=\"label\")\n",
    "\n",
    "# Ajustar el indexer al DataFrame y transformar los datos\n",
    "train_encoded = indexer.fit(train_encoded).transform(train_encoded)\n",
    "test_encoded = indexer.fit(test_encoded).transform(test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Vectorizar las columnas predictoras\n",
    "assembler = VectorAssembler(inputCols=X, outputCol=\"features\")\n",
    "train_df = assembler.transform(train_encoded)\n",
    "test_df = assembler.transform(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.select('label', \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =  test_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresion Logistica Multivariante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Crear el modelo de regresión logística\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Entrenar el modelo\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions_lr = lr_model.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr.select(\"features\", \"label\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictions_lr = predictions_lr.withColumn(\"prediction\", F.col(\"prediction\").cast(\"double\"))\\\n",
    "                               .withColumn(\"label\", F.col(\"label\").cast(\"double\"))\n",
    "\n",
    "# Crear el evaluador\n",
    "evaluator_lr = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Evaluar la precisión\n",
    "accuracy_lr = evaluator_lr.evaluate(predictions_lr)\n",
    "\n",
    "print(f\"accuracy: {accuracy_lr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Hacer predicciones\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Ver los resultados\n",
    "rf_predictions.select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "\n",
    "# Asegurarse de que las columnas prediction y label están en double\n",
    "rf_predictions = rf_predictions.withColumn(\"prediction\", F.col(\"prediction\").cast(\"double\"))\\\n",
    "                               .withColumn(\"label\", F.col(\"label\").cast(\"double\"))\n",
    "\n",
    "# Crear el evaluador\n",
    "evaluator_rf = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Evaluar la precisión\n",
    "accuracy_rf = evaluator_rf.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"accuracy: {accuracy_rf}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Crear el modelo de Naive Bayes\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "nb_model = nb.fit(train_df)\n",
    "\n",
    "# Hacer predicciones\n",
    "nb_predictions = nb_model.transform(test_df)\n",
    "\n",
    "# Ver los resultados\n",
    "nb_predictions.select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "\n",
    "# Asegurarse de que las columnas prediction y label están en double\n",
    "nb_predictions = nb_predictions.withColumn(\"prediction\", F.col(\"prediction\").cast(\"double\"))\\\n",
    "                               .withColumn(\"label\", F.col(\"label\").cast(\"double\"))\n",
    "\n",
    "# Crear el evaluador\n",
    "evaluator_nb = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Evaluar la precisión\n",
    "accuracy_nb = evaluator_nb.evaluate(nb_predictions)\n",
    "\n",
    "print(f\"accuracy: {accuracy_nb}\")\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"file:/c:/Users/AngelClavellina/Documents/MLOps/TrafficViolationsMexicoCity/artifacts/stage03/train_preprocessing.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------+----------------------------------------------------------------------------------------+\n",
      "|label|categoria                       |StandardFeatures                                                                        |\n",
      "+-----+--------------------------------+----------------------------------------------------------------------------------------+\n",
      "|0.0  |USO INCORRECTO DE LA VIA PUBLICA|[0.1783865342247831,0.008355974918408293,1.2573596540620916E-4,0.054856414994287214,7.0]|\n",
      "|0.0  |USO INCORRECTO DE LA VIA PUBLICA|[0.14614345926974737,0.7574525893408702,0.004291423167124965,0.2353230594293774,13.0]   |\n",
      "|1.0  |DOCUMENTACION INCOMPLETA        |[0.12902423424063678,0.7574525893408702,2.733390552308895E-6,0.08690815261066132,24.0]  |\n",
      "|1.0  |DOCUMENTACION INCOMPLETA        |[0.14614345926974737,0.7574525893408702,3.0067296075397847E-5,0.08690815261066132,1.0]  |\n",
      "|1.0  |DOCUMENTACION INCOMPLETA        |[0.14614345926974737,0.7574525893408702,1.1753579374928248E-4,0.08690815261066132,1.0]  |\n",
      "|1.0  |DOCUMENTACION INCOMPLETA        |[0.14614345926974737,0.7574525893408702,1.1753579374928248E-4,0.08690815261066132,1.0]  |\n",
      "|1.0  |DOCUMENTACION INCOMPLETA        |[0.14614345926974737,0.7574525893408702,6.25946436478737E-4,0.008552779038174533,1.0]   |\n",
      "|1.0  |DOCUMENTACION INCOMPLETA        |[0.14614345926974737,0.7574525893408702,6.25946436478737E-4,0.008552779038174533,1.0]   |\n",
      "|1.0  |DOCUMENTACION INCOMPLETA        |[0.14614345926974737,0.7574525893408702,6.25946436478737E-4,0.008552779038174533,1.0]   |\n",
      "|2.0  |SEGURIDAD VIAL                  |[0.14614345926974737,0.7574525893408702,2.733390552308895E-6,0.01328154469366892,1.0]   |\n",
      "+-----+--------------------------------+----------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df.select('label', 'categoria' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = aux.count()\n",
    "\n",
    "freq_col = df.select('categoria')\\\n",
    "                 .groupBy('categoria')\\\n",
    "                 .count()\\\n",
    "                 .select('categoria', (F.col('count') / total_count).alias('categoria_freq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------+\n",
      "|categoria                       |categoria_freq      |\n",
      "+--------------------------------+--------------------+\n",
      "|USO INCORRECTO DE LA VIA PUBLICA|0.8481656216003455  |\n",
      "|EXCESO DE VELOCIDAD             |0.011395505212575784|\n",
      "|INFRACCIONES DE TRANSITO        |0.008200171656926686|\n",
      "|CONDUCIR BAJO INFLUENCIA        |2.733390552308895E-5|\n",
      "|SEGURIDAD VIAL                  |0.043597579309326874|\n",
      "|DOCUMENTACION INCOMPLETA        |0.08861378831530206 |\n",
      "+--------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_col.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_fila = df.select([((F.sum(F.col(c).isNull().cast(T.IntegerType())) / df.count())).alias(c) for c in df.columns]).first()\n",
    "primer_fila_dict = {col: primer_fila[col] for col in df.columns if (primer_fila[col]<=1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primer_fila_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
